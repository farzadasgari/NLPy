{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32bf747-49d6-4a6d-9096-f4dc3ff3c0ed",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units called tokens. These tokens can be words, subwords, or characters. Tokenization is a crucial first step in many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52baa240-275c-4bf7-9118-71b808660b9f",
   "metadata": {},
   "source": [
    "## Why is Tokenization Important?\n",
    "\n",
    "Tokenization enables us to break down complex text into manageable pieces, making it easier to analyze and process. It serves as the foundation for many NLP tasks, such as parsing, part-of-speech tagging, and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f31615-4592-41c3-8650-84f8df778118",
   "metadata": {},
   "source": [
    "## Types of Tokenization\n",
    "\n",
    "1. **Word Tokenization**:\n",
    "   - Splitting text into individual words.\n",
    "   \n",
    "2. **Sentence Tokenization**:\n",
    "   - Splitting text into individual sentences.\n",
    "\n",
    "3. **Subword Tokenization**:\n",
    "   - Splitting text into smaller units than words, often used in advanced NLP models like BERT and GPT.\n",
    "\n",
    "4. **Character Tokenization**:\n",
    "   - Splitting text into individual characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc331f8-f0c9-4b2f-9692-d0124005342e",
   "metadata": {},
   "source": [
    "## Libraries for Tokenization\n",
    "\n",
    "Several libraries can help with tokenization in Python:\n",
    "- **NLTK**: Natural Language Toolkit\n",
    "- **spaCy**: Industrial-strength NLP\n",
    "- **Hugging Face Tokenizers**: Fast and efficient tokenizers used in transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb85c2c-8fa3-4ef4-a3bf-876105f097d9",
   "metadata": {},
   "source": [
    "## Example: Tokenizing Text with NLTK\n",
    "\n",
    "We'll start with NLTK, one of the most popular NLP libraries in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e89ffc3-cb9b-459d-809b-5820ddebefa2",
   "metadata": {},
   "source": [
    "### Word Tokenization with Vanilla Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a46d323-8b4c-46d1-90c1-5f4ebf360436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello,', 'I', 'am', 'Farzad', 'Asgari,', 'and', 'welcome', 'to', 'the', 'NLPy', 'course.', 'We', 'will', 'learn', 'a', 'lot', 'about', 'NLP!']\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text to be tokenized\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP!\"\n",
    "\n",
    "# Use the split() method to tokenize the text into words\n",
    "# By default, split() splits the text based on whitespace\n",
    "words = text.split()\n",
    "\n",
    "# Print the resulting list of word tokens\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e3fd0-4b3d-4254-b030-767d3af59c73",
   "metadata": {},
   "source": [
    "### Sentence Tokenization with Vanilla Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4833bf88-62f8-497d-a1cf-5a7bc1fecb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Hello, I am Farzad Asgari, and welcome to the NLPy course', 'We will learn a lot about NLP!']\n"
     ]
    }
   ],
   "source": [
    "# Define a sample text to be tokenized into sentences\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP!\"\n",
    "\n",
    "# Use the split() method to tokenize the text into sentences\n",
    "# Split based on periods followed by optional whitespace\n",
    "sentences = text.split('. ')\n",
    "\n",
    "# Print the resulting list of sentence tokens\n",
    "print(\"Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ad9b9-4ee0-46c3-a8b8-c5e9f246d011",
   "metadata": {},
   "source": [
    "### Word Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93088706-8b39-4f75-aa46-d7cf2a4a6895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello', ',', 'I', 'am', 'Farzad', 'Asgari', ',', 'and', 'welcome', 'to', 'the', 'NLPy', 'course', '.', 'We', 'will', 'learn', 'a', 'lot', 'about', 'NLP', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\free\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries from NLTK\n",
    "import nltk\n",
    "\n",
    "# Download the 'punkt' tokenizer models.\n",
    "# 'punkt' is a pre-trained model used for tokenizing words and sentences.\n",
    "# This model is necessary for the word_tokenize and sent_tokenize functions to work correctly.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Import the word_tokenize function from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a sample text to be tokenized\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP!\"\n",
    "\n",
    "# Use the word_tokenize function to split the text into individual words (tokens)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Print the resulting list of word tokens\n",
    "print(\"Word Tokens:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2135ee-427e-4574-b005-d7425c7ae252",
   "metadata": {},
   "source": [
    "### Sentence Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295b6437-fd5f-4d82-9702-2a69d012f292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Hello, I am Farzad Asgari, and welcome to the NLPy course.', 'We will learn a lot about NLP!']\n"
     ]
    }
   ],
   "source": [
    "# Import the sent_tokenize function from NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Define a sample text to be tokenized into sentences\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP!\"\n",
    "\n",
    "# Use the sent_tokenize function to split the text into individual sentences (tokens)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print the resulting list of sentence tokens\n",
    "print(\"Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2be7e-6dce-4c62-b7ae-e96aaa580551",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96579cdf-85d8-40b0-83c3-d544d8ac5020",
   "metadata": {},
   "source": [
    "#### Downloading spaCy's English Language Model\n",
    "\n",
    "To use spaCy for natural language processing tasks, you need to download a language model. The command below downloads the small English language model (`en_core_web_sm`) for spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5053e9-53a9-4806-b86e-c83bc8062377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 929.6 kB/s eta 0:00:14\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.0 MB/s eta 0:00:11\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.1 MB/s eta 0:00:10\n",
      "     ------- ------------------------------- 2.6/12.8 MB 909.5 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.6/12.8 MB 909.5 kB/s eta 0:00:12\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.1 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.1 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.1 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.1 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.1 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.1 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 9.7/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 9.7/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 10.5/12.8 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 10.7/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.8/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\work\\github\\nlpy\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7082dfa3-bc16-42b7-bf65-f5b65954d59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokens: ['Hello', ',', 'I', 'am', 'Farzad', 'Asgari', ',', 'and', 'welcome', 'to', 'the', 'NLPy', 'course', '.', 'We', 'will', 'learn', 'a', 'lot', 'about', 'NLP', '!']\n"
     ]
    }
   ],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "# 'en_core_web_sm' is a small English language model provided by spaCy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with the spaCy model to create a Doc object\n",
    "# The Doc object is a container for the processed text and its annotations\n",
    "doc = nlp(\"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP!\")\n",
    "\n",
    "# Extract tokens (words) from the Doc object\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Print the resulting list of tokens\n",
    "print(\"spaCy Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c6162-24b4-4878-b570-dfbb61de59a8",
   "metadata": {},
   "source": [
    "### Sentence Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6dc9bd-c3f9-4e5d-9e70-7a89087db133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Tokens: ['Hello, I am Farzad Asgari, and welcome to the NLPy course.', 'We will learn a lot about NLP!']\n"
     ]
    }
   ],
   "source": [
    "# Continue using the Doc object from the previous example\n",
    "# Extract sentences from the Doc object\n",
    "sentences = [sent.text for sent in list(doc.sents)]\n",
    "\n",
    "# Print the resulting list of sentence tokens\n",
    "# Each sentence is represented as a string\n",
    "print(\"spaCy Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc37faa",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Tokenization is a fundamental step in NLP, breaking down text into manageable units. Depending on the task and the model, different types of tokenization may be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
