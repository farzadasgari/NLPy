{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285f764a-9f52-4110-a018-b31487562b53",
   "metadata": {},
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "Stopwords are common words that are often filtered out from text during preprocessing because they carry less meaningful information compared to other words. Examples include \"the,\" \"is,\" \"in,\" and \"and.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c624a0d-7b59-4225-9562-8c63bd24dfb1",
   "metadata": {},
   "source": [
    "## Why Remove Stopwords?\n",
    "\n",
    "Removing stopwords helps reduce the dimensionality of the text and improves the efficiency and effectiveness of many NLP tasks by focusing on more meaningful words.\n",
    "\n",
    "## Libraries for Stopword Removal\n",
    "\n",
    "Several libraries provide built-in stopwords lists:\n",
    "- **NLTK**: Natural Language Toolkit\n",
    "- **spaCy**: Industrial-strength NLP\n",
    "- **Scikit-learn**: Machine learning library with built-in stopwords list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0c0ab-84e7-42af-8382-d81aa3f4f713",
   "metadata": {},
   "source": [
    "### Removing Stopwords with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d520c1-96f7-44c3-893b-dc57c338cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Hello', ',', 'Farzad', 'Asgari', ',', 'welcome', 'NLPy', 'course', '.', 'learn', 'lot', 'NLP', 'data', 'science', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\free\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\free\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords corpus if not already installed\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a sample text\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP and data science.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Load the list of stopwords in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from the tokenized words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered list of words\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7090f77-6f10-42cf-9658-7e86d2c33032",
   "metadata": {},
   "source": [
    "### Removing Stopwords with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c55514-a504-4cbc-b4d4-611b3a361ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Hello', ',', 'Farzad', 'Asgari', ',', 'welcome', 'NLPy', 'course', '.', 'learn', 'lot', 'NLP', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a sample text\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP and data science.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stopwords from the tokens\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Print the filtered list of words\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872973c-77dc-404f-94af-93502f2d8627",
   "metadata": {},
   "source": [
    "### Removing Stopwords with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3433e656-83cf-4348-a54d-6cffcaa4bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Hello,', 'Farzad', 'Asgari,', 'welcome', 'NLPy', 'course.', 'learn', 'lot', 'NLP', 'data', 'science.']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define a sample text\n",
    "text = \"Hello, I am Farzad Asgari, and welcome to the NLPy course. We will learn a lot about NLP and data science.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Remove stopwords using scikit-learn's ENGLISH_STOP_WORDS\n",
    "filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Print the filtered list of words\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d8fd9-6adb-4f46-809c-0c22220c9a4f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Removing stopwords is an important step in text preprocessing that helps focus on the most meaningful words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
